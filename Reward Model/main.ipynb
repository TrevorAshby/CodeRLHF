{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from trl import RewardConfig, RewardTrainer\n",
    "from peft import LoraConfig, TaskType # Parameter Efficient Fine Tuning\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Weights and Biases for training logging\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TRAIN = pd.read_feather(\"../mini_codenet/data/split/reward_train.ftr\")\n",
    "DATASET_EVAL = pd.read_feather(\"../mini_codenet/data/split/reward_val.ftr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TRAIN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(DATASET_TRAIN))\n",
    "DATASET_TRAIN.groupby(\"status\")[\"solution\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_EVAL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(DATASET_EVAL))\n",
    "DATASET_EVAL.groupby(\"status\")[\"solution\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000 accepted solutions at random.\n",
    "accepted_train = DATASET_TRAIN[DATASET_TRAIN[\"status\"] == \"Accepted\"][[\"submission_id\", \"problem_id\", \"language\", \"solution\"]]\n",
    "rejected_train = DATASET_TRAIN[DATASET_TRAIN[\"status\"] != \"Accepted\"][[\"submission_id\", \"problem_id\", \"language\", \"solution\"]]\n",
    "accepted_eval = DATASET_EVAL[DATASET_EVAL[\"status\"] == \"Accepted\"][[\"submission_id\", \"problem_id\", \"language\", \"solution\"]]\n",
    "rejected_eval = DATASET_EVAL[DATASET_EVAL[\"status\"] != \"Accepted\"][[\"submission_id\", \"problem_id\", \"language\", \"solution\"]]\n",
    "\n",
    "print(\"Total Accepted Problems in TRAIN:\", len(accepted_train[\"submission_id\"]))\n",
    "print(\"Total Rejected Problems in TRAIN:\", len(rejected_train[\"submission_id\"]))\n",
    "print(\"Unique IDs in Accepted TRAIN:\", len(accepted_train[\"problem_id\"].unique()))\n",
    "print(\"Unique IDs in Rejected TRAIN:\", len(rejected_train[\"problem_id\"].unique()))\n",
    "print(\"------------\")\n",
    "print(\"Total Accepted Problems in EVAL:\", len(accepted_eval[\"submission_id\"]))\n",
    "print(\"Total Rejected Problems in EVAL:\", len(rejected_eval[\"submission_id\"]))\n",
    "print(\"Unique IDs in Accepted EVAL:\", len(accepted_eval[\"problem_id\"].unique()))\n",
    "print(\"Unique IDs in Rejected EVAL:\", len(rejected_eval[\"problem_id\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each accepted solution, chose a contrasting rejected \n",
    "def get_contrastive_pairs(data_accepted, data_rejected, n=10):\n",
    "    data = { \"accepted\": [], \"rejected\": [] }\n",
    "\n",
    "    # SPEED UP!! Group rejected answers by problem_id and language and cache the results so\n",
    "    # we do not have to filter the whole dataset inside the main for-loop on every iteration.\n",
    "    # Plus, we get O(1) look up time ðŸ˜Ž\n",
    "    grouped_rejected = data_rejected.groupby([\"problem_id\", \"language\"])[\"solution\"].apply(list).to_dict()\n",
    "\n",
    "    for _, accepted_pid, accepted_lang, accepted_sol in tqdm(data_accepted.values):\n",
    "        key = (accepted_pid, accepted_lang)\n",
    "\n",
    "        if key in grouped_rejected:\n",
    "            # Get up to `n`` rejected examples in the current language for the current problem.\n",
    "            rejected_filtered = grouped_rejected[key][:n]\n",
    "\n",
    "            for rejected_sol in rejected_filtered:\n",
    "                data[\"accepted\"].append(accepted_sol)\n",
    "                data[\"rejected\"].append(rejected_sol)\n",
    "        else:\n",
    "            # The problem only contains a correct solutions in the current language. Skip it.\n",
    "            pass\n",
    "\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "# Tokenize chosen/rejected pairs of inputs\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    new_examples = {\n",
    "        \"input_ids_chosen\": [],\n",
    "        \"attention_mask_chosen\": [],\n",
    "        \"input_ids_rejected\": [],\n",
    "        \"attention_mask_rejected\": [],\n",
    "    }\n",
    "\n",
    "    for chosen, rejected in zip(examples[\"accepted\"], examples[\"rejected\"]):\n",
    "        tokenized_chosen = tokenizer(chosen)\n",
    "        tokenized_rejected = tokenizer(rejected)\n",
    "\n",
    "        new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n",
    "        new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n",
    "\n",
    "    return new_examples\n",
    "\n",
    "# Preprocess the dataset and filter out examples that are longer than args.max_length\n",
    "def process_data(accepted, rejected, tokenizer, args):\n",
    "    dataset = get_contrastive_pairs(accepted, rejected)\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda example: preprocess_function(example, tokenizer),\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "    )\n",
    "\n",
    "    dataset = dataset.filter(\n",
    "        lambda x: len(x[\"input_ids_chosen\"]) <= args.reward_config.max_length\n",
    "        and len(x[\"input_ids_rejected\"]) <= args.reward_config.max_length\n",
    "    )\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    model_name: str = \"../hf_model/\" # TODO: Change path to correct SFT model\n",
    "    \"\"\"the model name\"\"\"\n",
    "    eval_split: bool = False\n",
    "    \"\"\"the dataset split to evaluate on; default to 'none' (no evaluation)\"\"\"\n",
    "    reward_config: RewardConfig = field(\n",
    "        default_factory=lambda: RewardConfig(\n",
    "            output_dir=\"output\",\n",
    "            per_device_train_batch_size=64,\n",
    "            num_train_epochs=10,\n",
    "            gradient_accumulation_steps=16,\n",
    "            gradient_checkpointing=True,\n",
    "            gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "            learning_rate=1.41e-5,\n",
    "            report_to=\"wandb\", # log training progress to Weights and Biases\n",
    "            remove_unused_columns=False,\n",
    "            optim=\"adamw_torch\",\n",
    "            logging_steps=500,\n",
    "            evaluation_strategy=\"no\",\n",
    "            max_length=256, # TODO: NEED TO CHANGE THIS!\n",
    "        )\n",
    "    )\n",
    "\n",
    "args = ScriptArguments()\n",
    "args.reward_config.evaluation_strategy = \"steps\" if args.eval_split else \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset and pre-process it\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data:\")\n",
    "train_dataset = process_data(accepted_train, rejected_train, tokenizer, args)\n",
    "\n",
    "print(\"\\nEvaluation Data:\")\n",
    "eval_dataset = process_data(accepted_eval, rejected_eval, tokenizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=1)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define the Trainer\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args.reward_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
