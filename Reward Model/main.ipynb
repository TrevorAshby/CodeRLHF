{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from trl import RewardConfig, RewardTrainer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Weights and Biases for training logging\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace this with correct data split\n",
    "DATASET = pd.read_feather(\"../mini_codenet/en_mini_codenet.ftr\")\n",
    "DATASET.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000 accepted solutions at random.\n",
    "# TODO: In practice, to use all of our data, we would consider all accepted solutions\n",
    "subset_accepted = DATASET[DATASET[\"status\"] == \"Accepted\"].sample(1000)[[\"submission_id\", \"problem_id\"]]\n",
    "\n",
    "print(\"Total Problems:\", len(subset_accepted[\"submission_id\"]))\n",
    "print(\"Unique IDs:\", len(subset_accepted[\"problem_id\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each accepted solution, chose a contrasting rejected solution at random\n",
    "# TODO: In practice, to use all of our data, we would consider all accepted-rejected pairs per problem id\n",
    "data = { \"accepted\": [], \"rejected\": [] }\n",
    "for submission_id, problem_id in tqdm(subset_accepted.values):\n",
    "    accepted = DATASET[DATASET[\"submission_id\"] == submission_id]\n",
    "    rejected = DATASET[(DATASET[\"problem_id\"] == problem_id) & (DATASET[\"status\"] != \"Accepted\")].sample(1)\n",
    "\n",
    "    data[\"accepted\"].append(accepted[\"solution\"].iloc[0])\n",
    "    data[\"rejected\"].append(rejected[\"solution\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    model_name: str = \"../hf_model/\" # TODO: Change path to correct SFT model\n",
    "    \"\"\"the model name\"\"\"\n",
    "    eval_split: bool = False\n",
    "    \"\"\"the dataset split to evaluate on; default to 'none' (no evaluation)\"\"\"\n",
    "    reward_config: RewardConfig = field(\n",
    "        default_factory=lambda: RewardConfig(\n",
    "            output_dir=\"output\",\n",
    "            per_device_train_batch_size=64,\n",
    "            num_train_epochs=1,\n",
    "            gradient_accumulation_steps=16,\n",
    "            gradient_checkpointing=True,\n",
    "            gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "            learning_rate=1.41e-5,\n",
    "            report_to=\"wandb\", # log training progress to Weights and Biases\n",
    "            remove_unused_columns=False,\n",
    "            optim=\"adamw_torch\",\n",
    "            logging_steps=500,\n",
    "            evaluation_strategy=\"no\",\n",
    "            max_length=256, # TODO: NEED TO CHANGE THIS!\n",
    "        )\n",
    "    )\n",
    "\n",
    "args = ScriptArguments()\n",
    "args.reward_config.evaluation_strategy = \"steps\" if args.eval_split else \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset and pre-process it\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "train_dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=1)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize chosen/rejected pairs of inputs\n",
    "def preprocess_function(examples):\n",
    "    new_examples = {\n",
    "        \"input_ids_chosen\": [],\n",
    "        \"attention_mask_chosen\": [],\n",
    "        \"input_ids_rejected\": [],\n",
    "        \"attention_mask_rejected\": [],\n",
    "    }\n",
    "    for chosen, rejected in zip(examples[\"accepted\"], examples[\"rejected\"]):\n",
    "        tokenized_chosen = tokenizer(chosen)\n",
    "        tokenized_rejected = tokenizer(rejected)\n",
    "\n",
    "        new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n",
    "        new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n",
    "\n",
    "    return new_examples\n",
    "\n",
    "\n",
    "# Preprocess the dataset and filter out examples that are longer than args.max_length\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    ")\n",
    "train_dataset = train_dataset.filter(\n",
    "    lambda x: len(x[\"input_ids_chosen\"]) <= args.reward_config.max_length\n",
    "    and len(x[\"input_ids_rejected\"]) <= args.reward_config.max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.eval_split:\n",
    "    eval_dataset = None\n",
    "else:\n",
    "    # TODO: Load evaluation split following a similar process to loading the training split\n",
    "\n",
    "    # eval_dataset = ...\n",
    "\n",
    "    # eval_dataset = eval_dataset.map(\n",
    "    #     preprocess_function,\n",
    "    #     batched=True,\n",
    "    #     num_proc=4,\n",
    "    # )\n",
    "\n",
    "    # eval_dataset = eval_dataset.filter(\n",
    "    #     lambda x: len(x[\"input_ids_chosen\"]) <= args.reward_config.max_length\n",
    "    #     and len(x[\"input_ids_rejected\"]) <= args.reward_config.max_length\n",
    "    # )\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define the Trainer\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args.reward_config,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
